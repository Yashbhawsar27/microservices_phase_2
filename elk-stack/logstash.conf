# Logstash Configuration for Microservices Log Processing
# This configuration processes logs from all microservices and sends to Elasticsearch

# Input Configuration - Where to read logs from
input {
  # File input - Read log files from microservices
  file {
    path => "E:/Downloads/microservices_phase_2/logs/*.log"  # Path to log files
    start_position => "beginning"  # Start reading from beginning of file
    sincedb_path => "NUL"         # Don't track file position (for learning)
  }
}

# Filter Configuration - Process and parse logs
filter {
  # Add service name based on file path
  if [path] =~ "order-service" {
    mutate { add_field => { "service" => "order-service" } }
  } else if [path] =~ "payment-service" {
    mutate { add_field => { "service" => "payment-service" } }
  } else if [path] =~ "gateway" {
    mutate { add_field => { "service" => "gateway" } }
  } else {
    mutate { add_field => { "service" => "unknown" } }
  }

  # Parse log message using Grok pattern
  grok {
    # Extract timestamp, trace ID, span ID, log level, logger, and message
    match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:traceId},%{DATA:spanId}\] %{LOGLEVEL:level} %{DATA:logger} - %{GREEDYDATA:log_message}" }
  }

  # Parse timestamp
  date {
    match => [ "timestamp", "yyyy-MM-dd HH:mm:ss" ]
  }
}

# Output Configuration - Where to send processed logs
output {
  # Send to Elasticsearch
  elasticsearch {
    hosts => ["localhost:9200"]  # Elasticsearch server
    index => "microservices-logs-%{+YYYY.MM.dd}"  # Daily index pattern
  }
  # Also output to console for debugging
  stdout { codec => rubydebug }
}